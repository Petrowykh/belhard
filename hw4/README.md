
**Элементы обучения с подкреплением**

Перед нами стоит задача загнать машину на гору, но сделать это сразу не удается, так как мощности мотора не хватает. Здесь нужен разбег с соседнего склона так, чтобы по инерции забраться на правый – к флажку.

Машинку, которой мы управляем, назовем *агентом*. А среду, в которой происходит ее перемещение вместе с законами физики, - *окружением*. Именно в таких терминах формулируются задачи обучения с подкреплением – современного научного направления, используемое для обучения поведения игровых персонажей, управления динамическими объектами (машинами, самолетами, кораблями и т.п.).

![image](https://github.com/Petrowykh/belhard/assets/31277728/fc6b5503-0907-49ac-8a2b-480927c006fb)

В равные промежутки времени на машинку можно оказывать одно из воздействий (действие):

- 0 – разгон влево;
- 1 – движение по инерции;
- 2 – разгон вправо.

Если на текущей итерации машинка не добралась до флажка, то вознаграждение будет равно r = -1. Как только машинка добирается до цели, алгоритм завершается и подсчитывается суммарное вознаграждение:

$$R = r1+r2+…+rN$$

Виртуально окружение 
```
env = gym.make('MountainCar-v0')
```
Шаг
```
observation, reward, done, info = env.step(1)
```

Здесь метод step() возвращает четыре объекта:

- observation – текущую информацию об окружении после выполнения действия;
- reward – вознаграждение за выполнение команды (обычно, возвращается значение -1 за каждую выполненную команду);
- done – флаг (True – если цель была достигнута; False – в противном случае);
- info – словарь с информацией для отладки (алгоритм не должен использовать эти данные).


Функция приспособленности
```
    if actionCounter < LENGTH_CHROM:
        score = 0 - (LENGTH_CHROM - actionCounter) / LENGTH_CHROM
    else:
        score = abs(observation[0] - FLAG_LOCATION)
```
В нашем коде решение нашлось на 117 шаге
